{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn.utils import spectral_norm\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.mps.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seeds()\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensorDataset(Dataset):\n",
    "    def __init__(self, X, y, session_ids, window_size=3):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Group data by session\n",
    "        session_groups = {}\n",
    "        for idx, session_id in enumerate(session_ids):\n",
    "            if session_id not in session_groups:\n",
    "                session_groups[session_id] = []\n",
    "            session_groups[session_id].append((X[idx], y[idx]))\n",
    "\n",
    "        # Create sequences within each session\n",
    "        for session_id, session_data in session_groups.items():\n",
    "            # Create sliding windows of walking steps\n",
    "            for i in range(len(session_data) - window_size + 1):\n",
    "                # Option 1: Concatenate the walking steps into a single sequence\n",
    "                # This creates a 2D tensor with shape [window_size * time_steps, num_sensors]\n",
    "                steps = [data[0] for data in session_data[i:i+window_size]]\n",
    "                window_X = np.vstack(steps)  # Stack vertically to create one long sequence\n",
    "                \n",
    "                # Use the label from the last step in the window\n",
    "                window_y = session_data[i+window_size-1][1]\n",
    "                \n",
    "                self.X.append(window_X)\n",
    "                self.y.append(window_y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_original = np.load('sensor_data.npy')  # Shape: (num_steps, time_steps, num_sensors)\n",
    "metadata = pd.read_csv('combined_metadata.csv')\n",
    "y = metadata['has_ms'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by session for sequence integrity while stratifying\n",
    "sessions = metadata['session_id'].values\n",
    "unique_sessions = np.unique(sessions)\n",
    "\n",
    "# Create a mapping of session_id to MS status\n",
    "session_to_ms_status = {}\n",
    "for session_id in unique_sessions:\n",
    "    # Get all rows for this session\n",
    "    session_mask = metadata['session_id'] == session_id\n",
    "    # If any row has MS, the whole session is labeled as MS\n",
    "    has_ms = any(metadata.loc[session_mask, 'has_ms'] == 1)\n",
    "    session_to_ms_status[session_id] = 1 if has_ms else 0\n",
    "\n",
    "# Create lists of session IDs by MS status\n",
    "ms_sessions = [s for s, status in session_to_ms_status.items() if status == 1]\n",
    "non_ms_sessions = [s for s, status in session_to_ms_status.items() if status == 0]\n",
    "\n",
    "# Perform stratified split on MS and non-MS sessions separately\n",
    "train_ms, temp_ms = train_test_split(ms_sessions, test_size=0.3, random_state=42, shuffle=True)\n",
    "train_non_ms, temp_non_ms = train_test_split(non_ms_sessions, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Further split temp sets into validation and test\n",
    "val_ms, test_ms = train_test_split(temp_ms, test_size=0.5, random_state=42, shuffle=True)\n",
    "val_non_ms, test_non_ms = train_test_split(temp_non_ms, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "# Combine MS and non-MS sessions for each split\n",
    "train_sessions = train_ms + train_non_ms\n",
    "val_sessions = val_ms + val_non_ms\n",
    "test_sessions = test_ms + test_non_ms\n",
    "\n",
    "train_indices = metadata['session_id'].isin(train_sessions)\n",
    "val_indices = metadata['session_id'].isin(val_sessions)\n",
    "test_indices = metadata['session_id'].isin(test_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample X to have 10 timesteps instead of 100\n",
    "X = X_original.copy()#reshape(X_original.shape[0], 25, -1, X_original.shape[2]).mean(axis=2)\n",
    "\n",
    "X_train, X_val, X_test = X[train_indices], X[val_indices], X[test_indices]\n",
    "y_train, y_val, y_test = y[train_indices], y[val_indices], y[test_indices]\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X.shape[2])).reshape(X_train.shape)\n",
    "X_val = scaler.transform(X_val.reshape(-1, X.shape[2])).reshape(X_val.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# Session IDs for reference\n",
    "train_sessions_ids = sessions[train_indices]\n",
    "val_sessions_ids = sessions[val_indices]\n",
    "test_sessions_ids = sessions[test_indices]\n",
    "\n",
    "window_size = 4\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SensorDataset(X_train, y_train, train_sessions_ids, window_size=window_size)\n",
    "val_dataset = SensorDataset(X_val, y_val, val_sessions_ids, window_size=window_size)\n",
    "test_dataset = SensorDataset(X_test, y_test, test_sessions_ids, window_size=window_size)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMGCNN(nn.Module):\n",
    "    def __init__(self, output_dim=2):\n",
    "        super(EMGCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=7, out_channels=32, kernel_size=5, dilation=2, padding=4)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, dilation=2, padding=4)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)  # Global Average Pooling\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.bn_fc = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(128, 64, batch_first=True, num_layers=2, bidirectional=True, dropout=0.3)\n",
    "        self.layer_norm = nn.LayerNorm(64 * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        x = self.dropout(F.relu(self.bn_fc(self.fc1(x))))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, output_size=2, dropout_rate=0.5):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten if not already flat\n",
    "        if x.dim() > 2:\n",
    "            x = x.view(x.size(0), -1)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_sensors=7, num_timesteps=10, window_size=4, output_size=2):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Calculate input size after reshape: [batch, sensors, timesteps * window_size]\n",
    "        self.num_sensors = num_sensors\n",
    "        self.timesteps_per_window = num_timesteps * window_size\n",
    "        \n",
    "        # Simple 1D convolution layers\n",
    "        self.conv1 = nn.Conv1d(num_sensors, 16, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        \n",
    "        # Calculate size after convolutions and pooling\n",
    "        self.flat_size = 32 * (self.timesteps_per_window // 4)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flat_size, 64)\n",
    "        self.bn_fc = nn.BatchNorm1d(64)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(64, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reshape input: [batch, 1, sensors*timesteps*window] -> [batch, sensors, timesteps*window]\n",
    "        x = x.view(-1, self.num_sensors, self.timesteps_per_window)\n",
    "        \n",
    "        # Apply convolutions\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, self.flat_size)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.dropout(F.relu(self.bn_fc(self.fc1(x))))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=7, hidden_size=128, num_classes=2, num_layers=3, dropout=0.3, bidirectional=False):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size, \n",
    "            hidden_size, \n",
    "            num_layers=2, \n",
    "            batch_first=True, \n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size * 2)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            hidden_size * 2, \n",
    "            hidden_size, \n",
    "            num_layers=2, \n",
    "            batch_first=True, \n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        fc_input_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        self.fc1 = nn.Linear(fc_input_size, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm1_out, _ = self.lstm1(x)\n",
    "        _, seq_len, _ = lstm1_out.size()\n",
    "        lstm1_out_reshaped = lstm1_out[:, -1, :].contiguous()\n",
    "        lstm1_out_bn = self.bn1(lstm1_out_reshaped)\n",
    "        lstm1_out_expanded = lstm1_out_bn.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        lstm2_out, _ = self.lstm2(lstm1_out_expanded)\n",
    "        x = lstm2_out[:, -1, :]\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, epochs=20, scheduler=None):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(avg_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        CE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-CE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * CE_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1756\n",
      "Epoch 2, Loss: 0.1611\n",
      "Epoch 3, Loss: 0.1587\n",
      "Epoch 4, Loss: 0.1582\n",
      "Epoch 5, Loss: 0.1576\n",
      "Epoch 6, Loss: 0.1578\n",
      "Epoch 7, Loss: 0.1577\n",
      "Epoch 8, Loss: 0.1577\n",
      "Epoch 9, Loss: 0.1572\n",
      "Epoch 10, Loss: 0.1567\n",
      "Epoch 11, Loss: 0.1566\n",
      "Epoch 12, Loss: 0.1566\n",
      "Epoch 13, Loss: 0.1571\n",
      "Epoch 14, Loss: 0.1587\n",
      "Epoch 15, Loss: 0.1593\n",
      "Epoch 16, Loss: 0.1586\n",
      "Epoch 17, Loss: 0.1592\n",
      "Epoch 18, Loss: 0.1591\n",
      "Epoch 19, Loss: 0.1584\n",
      "Epoch 20, Loss: 0.1572\n",
      "Epoch 21, Loss: 0.1577\n",
      "Epoch 22, Loss: 0.1574\n",
      "Epoch 23, Loss: 0.1566\n",
      "Epoch 24, Loss: 0.1564\n",
      "Epoch 25, Loss: 0.1556\n",
      "Epoch 26, Loss: 0.1553\n",
      "Epoch 27, Loss: 0.1560\n",
      "Epoch 28, Loss: 0.1557\n",
      "Epoch 29, Loss: 0.1549\n",
      "Epoch 30, Loss: 0.1548\n",
      "Epoch 31, Loss: 0.1544\n",
      "Epoch 32, Loss: 0.1549\n",
      "Epoch 33, Loss: 0.1536\n",
      "Epoch 34, Loss: 0.1536\n",
      "Epoch 35, Loss: 0.1537\n",
      "Epoch 36, Loss: 0.1528\n",
      "Epoch 37, Loss: 0.1522\n",
      "Epoch 38, Loss: 0.1532\n",
      "Epoch 39, Loss: 0.1525\n",
      "Epoch 40, Loss: 0.1523\n",
      "Epoch 41, Loss: 0.1533\n",
      "Epoch 42, Loss: 0.1548\n",
      "Epoch 43, Loss: 0.1524\n",
      "Epoch 44, Loss: 0.1530\n",
      "Epoch 45, Loss: 0.1519\n",
      "Epoch 46, Loss: 0.1521\n",
      "Epoch 47, Loss: 0.1525\n",
      "Epoch 48, Loss: 0.1517\n",
      "Epoch 49, Loss: 0.1514\n",
      "Epoch 50, Loss: 0.1519\n"
     ]
    }
   ],
   "source": [
    "ms_weight = len(y_train) / (2 * np.sum(y_train == 1))\n",
    "ms_weight = ms_weight * 0.9\n",
    "\n",
    "weights = torch.tensor([1.0, ms_weight], dtype=torch.float32).to(device)\n",
    "\n",
    "# Setup for your data dimensions\n",
    "num_sensors = 7\n",
    "timesteps_per_step = 10\n",
    "window_size = 3\n",
    "\n",
    "# Calculate total input size\n",
    "input_size = num_sensors * timesteps_per_step * window_size\n",
    "\n",
    "# Create the MLP with the correct input size\n",
    "model = LSTMModel(bidirectional=True).to(device)\n",
    "criterion = FocalLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# Train and evaluate\n",
    "train(model, train_loader, criterion, optimizer, epochs=50, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            output = model(X_batch)\n",
    "            probs = torch.softmax(output, dim=1)[:, 1]\n",
    "            predicted = (probs > 0.5).int()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds)\n",
    "    rec = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    print(f'Accuracy: {acc:.2f}')\n",
    "    print(f'Precision: {prec:.2f}')\n",
    "    print(f'Recall: {rec:.2f}')\n",
    "    print(f'F1 Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.63\n",
      "Precision: 1.00\n",
      "Recall: 0.05\n",
      "F1 Score: 0.09\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 98 features per sequence\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def extract_features(dataset):\n",
    "    \"\"\"Extract statistical features from time series data.\"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        X, y = dataset[i]\n",
    "        X = X.numpy()  # Convert from tensor to numpy\n",
    "        \n",
    "        # For each sequence, compute statistical features\n",
    "        sequence_features = []\n",
    "        \n",
    "        # Statistical features for each sensor\n",
    "        for j in range(X.shape[1]):  # For each sensor\n",
    "            sensor_data = X[:, j]\n",
    "            \n",
    "            # Basic statistics\n",
    "            sequence_features.extend([\n",
    "                np.mean(sensor_data),        # Mean\n",
    "                np.std(sensor_data),         # Standard deviation\n",
    "                np.min(sensor_data),         # Minimum\n",
    "                np.max(sensor_data),         # Maximum\n",
    "                np.median(sensor_data),      # Median\n",
    "                stats.skew(sensor_data),     # Skewness\n",
    "                stats.kurtosis(sensor_data), # Kurtosis\n",
    "                np.percentile(sensor_data, 25),  # 25th percentile\n",
    "                np.percentile(sensor_data, 75),  # 75th percentile\n",
    "                np.ptp(sensor_data),         # Range (peak-to-peak)\n",
    "            ])\n",
    "        \n",
    "        # Add features for trends/dynamics\n",
    "        for j in range(X.shape[1]):  # For each sensor\n",
    "            sensor_data = X[:, j]\n",
    "            if len(sensor_data) > 5:  # Need sufficient data points\n",
    "                # Linear trend\n",
    "                from scipy import signal\n",
    "                detrended = signal.detrend(sensor_data)\n",
    "                trend = sensor_data - detrended\n",
    "                sequence_features.append(np.mean(trend))\n",
    "                \n",
    "                # First and second derivatives\n",
    "                first_diff = np.diff(sensor_data)\n",
    "                sequence_features.extend([\n",
    "                    np.mean(np.abs(first_diff)),   # Mean absolute change\n",
    "                    np.std(first_diff),            # Variability of change\n",
    "                ])\n",
    "                \n",
    "                if len(first_diff) > 1:\n",
    "                    second_diff = np.diff(first_diff)\n",
    "                    sequence_features.append(np.mean(np.abs(second_diff)))  # Acceleration\n",
    "        \n",
    "        features.append(sequence_features)\n",
    "        labels.append(y.item())  # Convert from tensor to scalar\n",
    "    \n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Extract features\n",
    "X_train_features, y_train = extract_features(train_dataset)\n",
    "X_val_features, y_val = extract_features(val_dataset)\n",
    "X_test_features, y_test = extract_features(test_dataset)\n",
    "\n",
    "print(f\"Extracted {X_train_features.shape[1]} features per sequence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate different models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    \"XGBoost\": xgb.XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            objective='binary:logistic',\n",
    "            random_state=42\n",
    "        )\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Special handling for XGBoost to use early stopping\n",
    "    model.fit(X_train_features, y_train)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_val_pred = model.predict(X_val_features)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    precision = precision_score(y_val, y_val_pred)\n",
    "    recall = recall_score(y_val, y_val_pred)\n",
    "    f1 = f1_score(y_val, y_val_pred)\n",
    "    \n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1\n",
    "    }\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8959\n",
      "Precision: 0.8573\n",
      "Recall: 0.8256\n",
      "F1 Score: 0.8412\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92      1509\n",
      "           1       0.86      0.83      0.84       757\n",
      "\n",
      "    accuracy                           0.90      2266\n",
      "   macro avg       0.89      0.88      0.88      2266\n",
      "weighted avg       0.90      0.90      0.90      2266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_combined = np.vstack((X_train_features, X_val_features))\n",
    "y_combined = np.concatenate((y_train, y_val))\n",
    "\n",
    "# Best parameters: {'learning_rate': 0.2, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 1.0}\n",
    "optimized_gb = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.2,\n",
    "    max_depth=3,\n",
    "    min_samples_split=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "optimized_gb.fit(X_combined, y_combined)\n",
    "y_test_pred = optimized_gb.predict(X_test_features)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Results (5 folds):\n",
      "Accuracy: 0.9980 ± 0.0007\n",
      "Precision: 0.9974 ± 0.0023\n",
      "Recall: 0.9971 ± 0.0023\n",
      "F1 Score: 0.9972 ± 0.0009\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "\n",
    "# Combine training and validation data\n",
    "X_combined = np.vstack((X_train_features, X_val_features))\n",
    "y_combined = np.concatenate((y_train, y_val))\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    optimized_gb, \n",
    "    X_combined, \n",
    "    y_combined, \n",
    "    cv=cv,\n",
    "    scoring=['accuracy', 'precision', 'recall', 'f1']\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"Cross-Validation Results (5 folds):\")\n",
    "print(f\"Accuracy: {cv_results['test_accuracy'].mean():.4f} ± {cv_results['test_accuracy'].std():.4f}\")\n",
    "print(f\"Precision: {cv_results['test_precision'].mean():.4f} ± {cv_results['test_precision'].std():.4f}\")\n",
    "print(f\"Recall: {cv_results['test_recall'].mean():.4f} ± {cv_results['test_recall'].std():.4f}\")\n",
    "print(f\"F1 Score: {cv_results['test_f1'].mean():.4f} ± {cv_results['test_f1'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GB Regular CV:\n",
    "Accuracy: 0.8883 ± 0.0505\n",
    "Precision: 0.8854 ± 0.0345\n",
    "Recall: 0.7929 ± 0.1382\n",
    "F1 Score: 0.8308 ± 0.0894"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to 'optimized_gb_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model to a file\n",
    "with open('optimized_gb_model.pkl', 'wb') as file:\n",
    "    pickle.dump(optimized_gb, file)\n",
    "\n",
    "print(\"Model saved to 'optimized_gb_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy: 0.9095\n",
      "Ensemble Precision: 0.8710\n",
      "Ensemble Recall: 0.8560\n",
      "Ensemble F1 Score: 0.8634\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93      1509\n",
      "           1       0.87      0.86      0.86       757\n",
      "\n",
      "    accuracy                           0.91      2266\n",
      "   macro avg       0.90      0.90      0.90      2266\n",
      "weighted avg       0.91      0.91      0.91      2266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Combine training and validation sets\n",
    "X_combined = np.vstack((X_train_features, X_val_features))\n",
    "y_combined = np.concatenate((y_train, y_val))\n",
    "\n",
    "# Define the base estimator with your tuned parameters\n",
    "base_gb = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.2,\n",
    "    max_depth=3,\n",
    "    min_samples_split=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create an ensemble of multiple instances of your GradientBoostingClassifier\n",
    "ensemble_gb = BaggingClassifier(\n",
    "    estimator=base_gb,\n",
    "    n_estimators=5,          # number of copies\n",
    "    max_samples=0.8,         # each model trains on 80% of the combined data (bootstrapped)\n",
    "    bootstrap=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the ensemble on the combined training set\n",
    "ensemble_gb.fit(X_combined, y_combined)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_test_pred = ensemble_gb.predict_proba(X_test_features)[:, 1]\n",
    "y_test_pred = (y_test_pred >= 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Ensemble Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Ensemble Precision: {precision:.4f}\")\n",
    "print(f\"Ensemble Recall: {recall:.4f}\")\n",
    "print(f\"Ensemble F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy: 0.9210\n",
      "Ensemble Precision: 0.8595\n",
      "Ensemble Recall: 0.9128\n",
      "Ensemble F1 Score: 0.8853\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94      1509\n",
      "           1       0.86      0.91      0.89       757\n",
      "\n",
      "    accuracy                           0.92      2266\n",
      "   macro avg       0.91      0.92      0.91      2266\n",
      "weighted avg       0.92      0.92      0.92      2266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = ensemble_gb.predict_proba(X_test_features)[:, 1]\n",
    "y_test_pred = (y_test_pred >= 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Ensemble Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Ensemble Precision: {precision:.4f}\")\n",
    "print(f\"Ensemble Recall: {recall:.4f}\")\n",
    "print(f\"Ensemble F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
