{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from scipy import stats, signal\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.mps.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seeds()\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEnrichedDataset(Dataset):\n",
    "    def __init__(self, X, y, session_ids, metadata, window_size=3):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.metadata = metadata\n",
    "        self.session_ids = session_ids\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Group data by session\n",
    "        session_groups = {}\n",
    "        for idx, session_id in enumerate(session_ids):\n",
    "            session_groups.setdefault(session_id, []).append((X[idx], y[idx]))\n",
    "\n",
    "        # Create sliding windows within each session\n",
    "        for session_id, session_data in session_groups.items():\n",
    "            for i in range(len(session_data) - window_size + 1):\n",
    "                steps = [data[0] for data in session_data[i:i+window_size]]\n",
    "                window_X = np.vstack(steps)\n",
    "                window_y = session_data[i+window_size-1][1]\n",
    "                self.X.append(window_X)\n",
    "                self.y.append(window_y)\n",
    "\n",
    "        # Now extract features for all windows, then append duration\n",
    "        self.X, self.y = self.extract_features()\n",
    "    \n",
    "    def extract_features(self):\n",
    "        features = []\n",
    "        labels = []\n",
    "        for i in range(len(self.X)):\n",
    "            X_win, y_val = self.X[i], self.y[i]\n",
    "            sequence_features = []\n",
    "            # Compute basic statistical features for each sensor\n",
    "            for j in range(X_win.shape[1]):\n",
    "                sensor_data = X_win[:, j]\n",
    "                freqs, psd = signal.welch(sensor_data, fs=100)\n",
    "                sequence_features.extend([\n",
    "                    np.mean(sensor_data),\n",
    "                    np.std(sensor_data),\n",
    "                    np.min(sensor_data),\n",
    "                    np.max(sensor_data),\n",
    "                    np.median(sensor_data),\n",
    "                    stats.skew(sensor_data),\n",
    "                    stats.kurtosis(sensor_data),\n",
    "                    np.percentile(sensor_data, 25),\n",
    "                    np.percentile(sensor_data, 75),\n",
    "                    np.ptp(sensor_data),\n",
    "                    np.sum(psd),\n",
    "                    np.mean(psd),\n",
    "                    np.max(psd),\n",
    "                    freqs[np.argmax(psd)]\n",
    "                ])\n",
    "            # Compute trend and dynamics features for each sensor\n",
    "            for j in range(X_win.shape[1]):\n",
    "                sensor_data = X_win[:, j]\n",
    "                if len(sensor_data) > 5:\n",
    "                    detrended = signal.detrend(sensor_data)\n",
    "                    trend = sensor_data - detrended\n",
    "                    sequence_features.append(np.mean(trend))\n",
    "                    first_diff = np.diff(sensor_data)\n",
    "                    sequence_features.extend([\n",
    "                        np.mean(np.abs(first_diff)),\n",
    "                        np.std(first_diff)\n",
    "                    ])\n",
    "                    if len(first_diff) > 1:\n",
    "                        second_diff = np.diff(first_diff)\n",
    "                        sequence_features.append(np.mean(np.abs(second_diff)))\n",
    "            features.append(sequence_features)\n",
    "            labels.append(y_val.item())\n",
    "        \n",
    "        # Append the session-level duration as a new feature for all samples\n",
    "        features = self.add_mean_duration_feature(features)\n",
    "        return np.array(features), np.array(labels)\n",
    "    \n",
    "    def add_mean_duration_feature(self, X_features):\n",
    "        # Use the same number of session_ids as there are samples\n",
    "        session_ids = self.session_ids[:len(X_features)]\n",
    "        mean_durations = np.zeros(len(X_features))\n",
    "        for session_id in np.unique(session_ids):\n",
    "            session_mask = session_ids == session_id\n",
    "            session_duration = self.metadata.loc[self.metadata['session_id'] == session_id, 'duration'].mean()\n",
    "            mean_durations[session_mask] = session_duration\n",
    "        mean_durations = mean_durations.reshape(-1, 1)\n",
    "        return np.hstack((X_features, mean_durations))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_sizes=[256, 128, 64], dropout=0.4, num_classes=2):\n",
    "        super(MLPModel, self).__init__()\n",
    "        \n",
    "        # Calculate flattened input size\n",
    "        flattened_size = input_shape[0]\n",
    "        \n",
    "        # Build the hidden layers\n",
    "        layers = []\n",
    "        prev_size = flattened_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        self.hidden_layers = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1], num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten the input\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        # Pass through hidden layers\n",
    "        x = self.hidden_layers(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a feature-enriched LSTM model\n",
    "class FeatureEnrichedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.3, num_classes=2):\n",
    "        super(FeatureEnrichedLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size*2, 64)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bn = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(\"x shape:\", x.shape)\n",
    "        \n",
    "        # x shape: [batch_size, seq_len, input_size]\n",
    "        \n",
    "        # LSTM output: [batch_size, seq_len, hidden_size*2]\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Use the last time step output\n",
    "        x = self.fc1(lstm_out[:, -1, :])\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, input_length, num_classes=2):\n",
    "        super(CNN1D, self).__init__()\n",
    "        # input_length is the length of the flattened feature vector from SensorDataset\n",
    "        \n",
    "        # Convolutional blocks\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.skip_adjust = nn.Conv1d(64, 128, kernel_size=1)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "        # Dynamically compute flattened size after convolutions using a dummy forward pass\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, input_length)  # shape: [batch, channels, length]\n",
    "            dummy_out = self._forward_conv(dummy_input)\n",
    "            self.flattened_size = dummy_out.view(1, -1).size(1)\n",
    "        \n",
    "        # Fully connected layers for classification\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def _forward_conv(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x1 = self.pool1(x)\n",
    "        x1 = self.dropout1(x1)\n",
    "        \n",
    "        skip = self.skip_adjust(x1)\n",
    "        skip = F.max_pool1d(skip, kernel_size=2, stride=2, padding=1)\n",
    "        \n",
    "        x = self.conv2(x1)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Adjust skip connection if dimensions differ\n",
    "        if skip.size(2) > x.size(2):\n",
    "            x = x + skip[:, :, :x.size(2)]\n",
    "        elif skip.size(2) < x.size(2):\n",
    "            x = x[:, :, :skip.size(2)] + skip\n",
    "        else:\n",
    "            x = x + skip\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, feature_length] from SensorDataset\n",
    "        x = x.unsqueeze(1)  # now [batch_size, 1, feature_length]\n",
    "        x = self._forward_conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device, epochs=50, scheduler=None):\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Print metrics for this epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_train_loss:.4f}')\n",
    "        \n",
    "        # Update learning rate if scheduler is provided\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(avg_train_loss)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    \n",
    "    print(\"\\nModel Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_original = np.load('sensor_data.npy')  # Shape: (num_steps, time_steps, num_sensors)\n",
    "metadata = pd.read_csv('combined_metadata.csv')\n",
    "y = metadata['has_ms'].values\n",
    "\n",
    "# Split by session for sequence integrity while stratifying\n",
    "sessions = metadata['session_id'].values\n",
    "unique_sessions = np.unique(sessions)\n",
    "\n",
    "# Create a mapping of session_id to MS status\n",
    "session_to_ms_status = {}\n",
    "for session_id in unique_sessions:\n",
    "    # Get all rows for this session\n",
    "    session_mask = metadata['session_id'] == session_id\n",
    "    # If any row has MS, the whole session is labeled as MS\n",
    "    has_ms = any(metadata.loc[session_mask, 'has_ms'] == 1)\n",
    "    session_to_ms_status[session_id] = 1 if has_ms else 0\n",
    "\n",
    "# Create lists of session IDs by MS status\n",
    "ms_sessions = [s for s, status in session_to_ms_status.items() if status == 1]\n",
    "non_ms_sessions = [s for s, status in session_to_ms_status.items() if status == 0]\n",
    "\n",
    "# Perform stratified split on MS and non-MS sessions separately\n",
    "train_ms, temp_ms = train_test_split(ms_sessions, test_size=0.3, random_state=42, shuffle=True)\n",
    "train_non_ms, temp_non_ms = train_test_split(non_ms_sessions, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Further split temp sets into validation and test\n",
    "val_ms, test_ms = train_test_split(temp_ms, test_size=0.5, random_state=42, shuffle=True)\n",
    "val_non_ms, test_non_ms = train_test_split(temp_non_ms, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "# Combine MS and non-MS sessions for each split\n",
    "train_sessions = train_ms + train_non_ms\n",
    "val_sessions = val_ms + val_non_ms\n",
    "test_sessions = test_ms + test_non_ms\n",
    "\n",
    "train_indices = metadata['session_id'].isin(train_sessions)\n",
    "val_indices = metadata['session_id'].isin(val_sessions)\n",
    "test_indices = metadata['session_id'].isin(test_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/talerez/Documents/AIPI_549/FinalProject/multiple-sclerosis-research/venv/lib/python3.9/site-packages/scipy/signal/_spectral_py.py:600: UserWarning: nperseg = 256 is greater than input length  = 100, using nperseg = 100\n",
      "  freqs, _, Pxy = _spectral_helper(x, y, fs, window, nperseg, noverlap,\n"
     ]
    }
   ],
   "source": [
    "# Downsample X to have 10 timesteps instead of 100\n",
    "X = X_original.reshape(X_original.shape[0], 25, -1, X_original.shape[2]).mean(axis=2)\n",
    "\n",
    "X_train, X_val, X_test = X[train_indices], X[val_indices], X[test_indices]\n",
    "y_train, y_val, y_test = y[train_indices], y[val_indices], y[test_indices]\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X.shape[2])).reshape(X_train.shape)\n",
    "X_val = scaler.transform(X_val.reshape(-1, X.shape[2])).reshape(X_val.shape)\n",
    "\n",
    "# Session IDs for reference\n",
    "train_sessions_ids = sessions[train_indices]\n",
    "val_sessions_ids = sessions[val_indices]\n",
    "\n",
    "window_size = 4\n",
    "\n",
    "# Code to set up and run the model\n",
    "# Prepare feature-enriched datasets\n",
    "train_dataset = FeatureEnrichedDataset(X_train, y_train, train_sessions_ids, metadata, window_size=window_size)\n",
    "val_dataset = FeatureEnrichedDataset(X_val, y_val, val_sessions_ids, metadata, window_size=window_size)\n",
    "\n",
    "# Prepare dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/talerez/Documents/AIPI_549/FinalProject/multiple-sclerosis-research/venv/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/50, Loss: 0.4910\n",
      "Epoch 2/50, Loss: 0.4255\n",
      "Epoch 3/50, Loss: 0.4171\n",
      "Epoch 4/50, Loss: 0.4102\n",
      "Epoch 5/50, Loss: 0.4050\n",
      "Epoch 6/50, Loss: 0.3971\n",
      "Epoch 7/50, Loss: 0.3912\n",
      "Epoch 8/50, Loss: 0.3890\n",
      "Epoch 9/50, Loss: 0.3858\n",
      "Epoch 10/50, Loss: 0.3803\n",
      "Epoch 11/50, Loss: 0.3800\n",
      "Epoch 12/50, Loss: 0.3750\n",
      "Epoch 13/50, Loss: 0.3702\n",
      "Epoch 14/50, Loss: 0.3634\n",
      "Epoch 15/50, Loss: 0.3647\n",
      "Epoch 16/50, Loss: 0.3552\n",
      "Epoch 17/50, Loss: 0.3492\n",
      "Epoch 18/50, Loss: 0.3487\n",
      "Epoch 19/50, Loss: 0.3425\n",
      "Epoch 20/50, Loss: 0.3430\n",
      "Epoch 21/50, Loss: 0.3413\n",
      "Epoch 22/50, Loss: 0.3395\n",
      "Epoch 23/50, Loss: 0.3331\n",
      "Epoch 24/50, Loss: 0.3278\n",
      "Epoch 25/50, Loss: 0.3306\n",
      "Epoch 26/50, Loss: 0.3311\n",
      "Epoch 27/50, Loss: 0.3231\n",
      "Epoch 28/50, Loss: 0.3265\n",
      "Epoch 29/50, Loss: 0.3235\n",
      "Epoch 30/50, Loss: 0.3244\n",
      "Epoch 31/50, Loss: 0.3206\n",
      "Epoch 32/50, Loss: 0.3184\n",
      "Epoch 33/50, Loss: 0.3172\n",
      "Epoch 34/50, Loss: 0.3230\n",
      "Epoch 35/50, Loss: 0.3213\n",
      "Epoch 36/50, Loss: 0.3138\n",
      "Epoch 37/50, Loss: 0.3072\n",
      "Epoch 38/50, Loss: 0.3173\n",
      "Epoch 39/50, Loss: 0.3115\n",
      "Epoch 40/50, Loss: 0.3137\n",
      "Epoch 41/50, Loss: 0.3034\n",
      "Epoch 42/50, Loss: 0.3060\n",
      "Epoch 43/50, Loss: 0.3037\n",
      "Epoch 44/50, Loss: 0.3097\n",
      "Epoch 45/50, Loss: 0.2995\n",
      "Epoch 46/50, Loss: 0.3029\n",
      "Epoch 47/50, Loss: 0.3035\n",
      "Epoch 48/50, Loss: 0.3063\n",
      "Epoch 49/50, Loss: 0.3013\n",
      "Epoch 50/50, Loss: 0.2963\n"
     ]
    }
   ],
   "source": [
    "input_length = train_dataset[0][0].shape[0]\n",
    "model = CNN1D(input_length=input_length, num_classes=2).to(device)\n",
    "\n",
    "# Define loss function with class weighting\n",
    "ms_weight = len(y_train) / (2 * np.sum(y_train == 1))\n",
    "ms_weight = ms_weight * 0.9\n",
    "\n",
    "weights = torch.tensor([1.0, ms_weight], dtype=torch.float32).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "model = train(\n",
    "    model, train_loader, \n",
    "    criterion, optimizer, device, \n",
    "    epochs=50, scheduler=scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance:\n",
      "Accuracy: 0.8045\n",
      "Precision: 0.7992\n",
      "Recall: 0.6590\n",
      "F1 Score: 0.7224\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.90      0.85      1460\n",
      "           1       0.80      0.66      0.72       918\n",
      "\n",
      "    accuracy                           0.80      2378\n",
      "   macro avg       0.80      0.78      0.79      2378\n",
      "weighted avg       0.80      0.80      0.80      2378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained model\n",
    "evaluate(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Performance:\n",
    "Accuracy: 0.8389\n",
    "Precision: 0.8078\n",
    "Recall: 0.7647\n",
    "F1 Score: 0.7857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/talerez/Documents/AIPI_549/FinalProject/multiple-sclerosis-research/venv/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75, Loss: 0.5014\n",
      "Epoch 2/75, Loss: 0.4158\n",
      "Epoch 3/75, Loss: 0.3861\n",
      "Epoch 4/75, Loss: 0.3623\n",
      "Epoch 5/75, Loss: 0.3484\n",
      "Epoch 6/75, Loss: 0.3361\n",
      "Epoch 7/75, Loss: 0.3266\n",
      "Epoch 8/75, Loss: 0.3206\n",
      "Epoch 9/75, Loss: 0.3102\n",
      "Epoch 10/75, Loss: 0.3059\n",
      "Epoch 11/75, Loss: 0.3036\n",
      "Epoch 12/75, Loss: 0.3072\n",
      "Epoch 13/75, Loss: 0.3027\n",
      "Epoch 14/75, Loss: 0.2983\n",
      "Epoch 15/75, Loss: 0.3026\n",
      "Epoch 16/75, Loss: 0.3021\n",
      "Epoch 17/75, Loss: 0.2935\n",
      "Epoch 18/75, Loss: 0.2999\n",
      "Epoch 19/75, Loss: 0.2966\n",
      "Epoch 20/75, Loss: 0.2966\n",
      "Epoch 21/75, Loss: 0.2838\n",
      "Epoch 22/75, Loss: 0.2885\n",
      "Epoch 23/75, Loss: 0.2923\n",
      "Epoch 24/75, Loss: 0.2945\n",
      "Epoch 25/75, Loss: 0.2892\n",
      "Epoch 26/75, Loss: 0.2885\n",
      "Epoch 27/75, Loss: 0.2851\n",
      "Epoch 28/75, Loss: 0.2774\n",
      "Epoch 29/75, Loss: 0.2798\n",
      "Epoch 30/75, Loss: 0.2684\n",
      "Epoch 31/75, Loss: 0.2767\n",
      "Epoch 32/75, Loss: 0.2686\n",
      "Epoch 33/75, Loss: 0.2673\n",
      "Epoch 34/75, Loss: 0.2680\n",
      "Epoch 35/75, Loss: 0.2751\n",
      "Epoch 36/75, Loss: 0.2701\n",
      "Epoch 37/75, Loss: 0.2677\n",
      "Epoch 38/75, Loss: 0.2684\n",
      "Epoch 39/75, Loss: 0.2655\n",
      "Epoch 40/75, Loss: 0.2606\n",
      "Epoch 41/75, Loss: 0.2673\n",
      "Epoch 42/75, Loss: 0.2623\n",
      "Epoch 43/75, Loss: 0.2649\n",
      "Epoch 44/75, Loss: 0.2619\n",
      "Epoch 45/75, Loss: 0.2551\n",
      "Epoch 46/75, Loss: 0.2562\n",
      "Epoch 47/75, Loss: 0.2603\n",
      "Epoch 48/75, Loss: 0.2623\n",
      "Epoch 49/75, Loss: 0.2658\n",
      "Epoch 50/75, Loss: 0.2576\n",
      "Epoch 51/75, Loss: 0.2515\n",
      "Epoch 52/75, Loss: 0.2589\n",
      "Epoch 53/75, Loss: 0.2562\n",
      "Epoch 54/75, Loss: 0.2579\n",
      "Epoch 55/75, Loss: 0.2559\n",
      "Epoch 56/75, Loss: 0.2592\n",
      "Epoch 57/75, Loss: 0.2580\n",
      "Epoch 58/75, Loss: 0.2573\n",
      "Epoch 59/75, Loss: 0.2469\n",
      "Epoch 60/75, Loss: 0.2496\n",
      "Epoch 61/75, Loss: 0.2550\n",
      "Epoch 62/75, Loss: 0.2453\n",
      "Epoch 63/75, Loss: 0.2442\n",
      "Epoch 64/75, Loss: 0.2404\n",
      "Epoch 65/75, Loss: 0.2410\n",
      "Epoch 66/75, Loss: 0.2501\n",
      "Epoch 67/75, Loss: 0.2463\n",
      "Epoch 68/75, Loss: 0.2396\n",
      "Epoch 69/75, Loss: 0.2350\n",
      "Epoch 70/75, Loss: 0.2417\n",
      "Epoch 71/75, Loss: 0.2436\n",
      "Epoch 72/75, Loss: 0.2445\n",
      "Epoch 73/75, Loss: 0.2405\n",
      "Epoch 74/75, Loss: 0.2401\n",
      "Epoch 75/75, Loss: 0.2415\n"
     ]
    }
   ],
   "source": [
    "input_size = train_dataset[0][0].shape[0]\n",
    "model = MLPModel(input_shape=(input_size,)).to(device)\n",
    "\n",
    "# Set up optimizer and criterion\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()#weight=weights)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# Train with your existing function\n",
    "model = train(model, train_loader, criterion, optimizer, device, epochs=75, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance:\n",
      "Accuracy: 0.7843\n",
      "Precision: 0.9571\n",
      "Recall: 0.4619\n",
      "F1 Score: 0.6231\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.99      0.85      1460\n",
      "           1       0.96      0.46      0.62       918\n",
      "\n",
      "    accuracy                           0.78      2378\n",
      "   macro avg       0.85      0.72      0.74      2378\n",
      "weighted avg       0.83      0.78      0.76      2378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained model\n",
    "evaluate(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Performance:\n",
    "Accuracy: 0.8192\n",
    "Precision: 0.8058\n",
    "Recall: 0.7004\n",
    "F1 Score: 0.7494\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.83      0.89      0.86      1460\n",
    "           1       0.81      0.70      0.75       918\n",
    "\n",
    "    accuracy                           0.82      2378\n",
    "   macro avg       0.82      0.80      0.80      2378\n",
    "weighted avg       0.82      0.82      0.82      2378"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
