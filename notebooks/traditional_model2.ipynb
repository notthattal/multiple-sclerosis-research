{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn.utils import spectral_norm\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.mps.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seeds()\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensorDataset(Dataset):\n",
    "    def __init__(self, X, y, session_ids, window_size=3):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Group data by session\n",
    "        session_groups = {}\n",
    "        for idx, session_id in enumerate(session_ids):\n",
    "            if session_id not in session_groups:\n",
    "                session_groups[session_id] = []\n",
    "            session_groups[session_id].append((X[idx], y[idx]))\n",
    "\n",
    "        # Create sequences within each session\n",
    "        for session_id, session_data in session_groups.items():\n",
    "            # Create sliding windows of walking steps\n",
    "            for i in range(len(session_data) - window_size + 1):\n",
    "                # Option 1: Concatenate the walking steps into a single sequence\n",
    "                # This creates a 2D tensor with shape [window_size * time_steps, num_sensors]\n",
    "                steps = [data[0] for data in session_data[i:i+window_size]]\n",
    "                window_X = np.vstack(steps)  # Stack vertically to create one long sequence\n",
    "                \n",
    "                # Use the label from the last step in the window\n",
    "                window_y = session_data[i+window_size-1][1]\n",
    "                \n",
    "                self.X.append(window_X)\n",
    "                self.y.append(window_y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_original = np.load('sensor_data.npy')  # Shape: (num_steps, time_steps, num_sensors)\n",
    "metadata = pd.read_csv('combined_metadata.csv')\n",
    "y = metadata['has_ms'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by session for sequence integrity while stratifying\n",
    "sessions = metadata['session_id'].values\n",
    "unique_sessions = np.unique(sessions)\n",
    "\n",
    "# Create a mapping of session_id to MS status\n",
    "session_to_ms_status = {}\n",
    "for session_id in unique_sessions:\n",
    "    # Get all rows for this session\n",
    "    session_mask = metadata['session_id'] == session_id\n",
    "    # If any row has MS, the whole session is labeled as MS\n",
    "    has_ms = any(metadata.loc[session_mask, 'has_ms'] == 1)\n",
    "    session_to_ms_status[session_id] = 1 if has_ms else 0\n",
    "\n",
    "# Create lists of session IDs by MS status\n",
    "ms_sessions = [s for s, status in session_to_ms_status.items() if status == 1]\n",
    "non_ms_sessions = [s for s, status in session_to_ms_status.items() if status == 0]\n",
    "\n",
    "# Perform stratified split on MS and non-MS sessions separately\n",
    "train_ms, temp_ms = train_test_split(ms_sessions, test_size=0.3, random_state=42, shuffle=True)\n",
    "train_non_ms, temp_non_ms = train_test_split(non_ms_sessions, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Further split temp sets into validation and test\n",
    "val_ms, test_ms = train_test_split(temp_ms, test_size=0.5, random_state=42, shuffle=True)\n",
    "val_non_ms, test_non_ms = train_test_split(temp_non_ms, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "# Combine MS and non-MS sessions for each split\n",
    "train_sessions = train_ms + train_non_ms\n",
    "val_sessions = val_ms + val_non_ms\n",
    "test_sessions = test_ms + test_non_ms\n",
    "\n",
    "train_indices = metadata['session_id'].isin(train_sessions)\n",
    "val_indices = metadata['session_id'].isin(val_sessions)\n",
    "test_indices = metadata['session_id'].isin(test_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample X to have 10 timesteps instead of 100\n",
    "X = X_original.copy()#reshape(X_original.shape[0], 25, -1, X_original.shape[2]).mean(axis=2)\n",
    "\n",
    "X_train, X_val, X_test = X[train_indices], X[val_indices], X[test_indices]\n",
    "y_train, y_val, y_test = y[train_indices], y[val_indices], y[test_indices]\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X.shape[2])).reshape(X_train.shape)\n",
    "X_val = scaler.transform(X_val.reshape(-1, X.shape[2])).reshape(X_val.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# Session IDs for reference\n",
    "train_sessions_ids = sessions[train_indices]\n",
    "val_sessions_ids = sessions[val_indices]\n",
    "test_sessions_ids = sessions[test_indices]\n",
    "\n",
    "window_size = 4\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SensorDataset(X_train, y_train, train_sessions_ids, window_size=window_size)\n",
    "val_dataset = SensorDataset(X_val, y_val, val_sessions_ids, window_size=window_size)\n",
    "test_dataset = SensorDataset(X_test, y_test, test_sessions_ids, window_size=window_size)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 126 features per sequence\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy import signal\n",
    "\n",
    "def extract_features(dataset):\n",
    "    \"\"\"Extract statistical features from time series data.\"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        X, y = dataset[i]\n",
    "        X = X.numpy()  # Convert from tensor to numpy\n",
    "        \n",
    "        # For each sequence, compute statistical features\n",
    "        sequence_features = []\n",
    "        \n",
    "        # Statistical features for each sensor\n",
    "        for j in range(X.shape[1]):  # For each sensor\n",
    "            sensor_data = X[:, j]\n",
    "            \n",
    "            # Basic statistics\n",
    "            freqs, psd = signal.welch(sensor_data, fs=100)\n",
    "\n",
    "            sequence_features.extend([\n",
    "                np.mean(sensor_data),        # Mean\n",
    "                np.std(sensor_data),         # Standard deviation\n",
    "                np.min(sensor_data),         # Minimum\n",
    "                np.max(sensor_data),         # Maximum\n",
    "                np.median(sensor_data),      # Median\n",
    "                stats.skew(sensor_data),     # Skewness\n",
    "                stats.kurtosis(sensor_data), # Kurtosis\n",
    "                np.percentile(sensor_data, 25),  # 25th percentile\n",
    "                np.percentile(sensor_data, 75),  # 75th percentile\n",
    "                np.ptp(sensor_data),         # Range (peak-to-peak)\n",
    "                np.sum(psd),            # Total power\n",
    "                np.mean(psd),           # Average power\n",
    "                np.max(psd),            # Peak frequency power\n",
    "                freqs[np.argmax(psd)]   # Dominant frequency\n",
    "            ])\n",
    "\n",
    "        # Add features for trends/dynamics\n",
    "        for j in range(X.shape[1]):  # For each sensor\n",
    "            sensor_data = X[:, j]\n",
    "            if len(sensor_data) > 5:  # Need sufficient data points\n",
    "                # Linear trend\n",
    "                detrended = signal.detrend(sensor_data)\n",
    "                trend = sensor_data - detrended\n",
    "                sequence_features.append(np.mean(trend))\n",
    "                \n",
    "                # First and second derivatives\n",
    "                first_diff = np.diff(sensor_data)\n",
    "                sequence_features.extend([\n",
    "                    np.mean(np.abs(first_diff)),   # Mean absolute change\n",
    "                    np.std(first_diff),            # Variability of change\n",
    "                ])\n",
    "                \n",
    "                if len(first_diff) > 1:\n",
    "                    second_diff = np.diff(first_diff)\n",
    "                    sequence_features.append(np.mean(np.abs(second_diff)))  # Acceleration\n",
    "        \n",
    "        features.append(sequence_features)\n",
    "        labels.append(y.item())  # Convert from tensor to scalar\n",
    "    \n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Extract features\n",
    "X_train_features, y_train = extract_features(train_dataset)\n",
    "X_val_features, y_val = extract_features(val_dataset)\n",
    "X_test_features, y_test = extract_features(test_dataset)\n",
    "\n",
    "print(f\"Extracted {X_train_features.shape[1]} features per sequence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10567, 100, 7)\n",
      "(10128, 126)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mean_duration_feature(X_features, metadata, session_ids):\n",
    "    # Filter session_ids to match the number of rows in X_features\n",
    "    session_ids = session_ids[:len(X_features)]\n",
    "\n",
    "    # Create an array to hold mean durations for each step\n",
    "    mean_durations = np.zeros(len(X_features))\n",
    "    \n",
    "    # Map session-level duration to each sample\n",
    "    for session_id in np.unique(session_ids):\n",
    "        session_mask = session_ids == session_id\n",
    "        session_duration = metadata.loc[metadata['session_id'] == session_id, 'duration'].mean()\n",
    "\n",
    "        # Fill mean duration for each sample in this session\n",
    "        mean_durations[session_mask] = session_duration\n",
    "\n",
    "    # Reshape for concatenation\n",
    "    mean_durations = mean_durations.reshape(-1, 1)\n",
    "    \n",
    "    # Append mean duration as a new feature\n",
    "    return np.hstack((X_features, mean_durations))\n",
    "\n",
    "# Corrected Data Integration\n",
    "X_train_features = add_mean_duration_feature(X_train_features, metadata, train_sessions_ids)\n",
    "X_val_features = add_mean_duration_feature(X_val_features, metadata, val_sessions_ids)\n",
    "X_test_features = add_mean_duration_feature(X_test_features, metadata, test_sessions_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9497\n",
      "Precision: 0.8906\n",
      "Recall: 0.9683\n",
      "F1 Score: 0.9278\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96      1509\n",
      "           1       0.89      0.97      0.93       757\n",
      "\n",
      "    accuracy                           0.95      2266\n",
      "   macro avg       0.94      0.95      0.94      2266\n",
      "weighted avg       0.95      0.95      0.95      2266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_combined = np.vstack((X_train_features, X_val_features))\n",
    "y_combined = np.concatenate((y_train, y_val))\n",
    "\n",
    "# Best parameters: {'learning_rate': 0.2, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 1.0}\n",
    "optimized_gb = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.2,\n",
    "    max_depth=3,\n",
    "    min_samples_split=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "optimized_gb.fit(X_combined, y_combined)\n",
    "y_test_pred = optimized_gb.predict(X_test_features)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Results (5 folds):\n",
      "Accuracy: 0.9980 ± 0.0007\n",
      "Precision: 0.9974 ± 0.0023\n",
      "Recall: 0.9971 ± 0.0023\n",
      "F1 Score: 0.9972 ± 0.0009\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "\n",
    "# Combine training and validation data\n",
    "X_combined = np.vstack((X_train_features, X_val_features))\n",
    "y_combined = np.concatenate((y_train, y_val))\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    optimized_gb, \n",
    "    X_combined, \n",
    "    y_combined, \n",
    "    cv=cv,\n",
    "    scoring=['accuracy', 'precision', 'recall', 'f1']\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"Cross-Validation Results (5 folds):\")\n",
    "print(f\"Accuracy: {cv_results['test_accuracy'].mean():.4f} ± {cv_results['test_accuracy'].std():.4f}\")\n",
    "print(f\"Precision: {cv_results['test_precision'].mean():.4f} ± {cv_results['test_precision'].std():.4f}\")\n",
    "print(f\"Recall: {cv_results['test_recall'].mean():.4f} ± {cv_results['test_recall'].std():.4f}\")\n",
    "print(f\"F1 Score: {cv_results['test_f1'].mean():.4f} ± {cv_results['test_f1'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GB Regular CV:\n",
    "Accuracy: 0.8883 ± 0.0505\n",
    "Precision: 0.8854 ± 0.0345\n",
    "Recall: 0.7929 ± 0.1382\n",
    "F1 Score: 0.8308 ± 0.0894"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to 'optimized_gb_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model to a file\n",
    "with open('optimized_gb_model.pkl', 'wb') as file:\n",
    "    pickle.dump(optimized_gb, file)\n",
    "\n",
    "print(\"Model saved to 'optimized_gb_model2.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy: 0.9612\n",
      "Ensemble Precision: 0.9166\n",
      "Ensemble Recall: 0.9723\n",
      "Ensemble F1 Score: 0.9436\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97      1509\n",
      "           1       0.92      0.97      0.94       757\n",
      "\n",
      "    accuracy                           0.96      2266\n",
      "   macro avg       0.95      0.96      0.96      2266\n",
      "weighted avg       0.96      0.96      0.96      2266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Combine training and validation sets\n",
    "X_combined = np.vstack((X_train_features, X_val_features))\n",
    "y_combined = np.concatenate((y_train, y_val))\n",
    "\n",
    "# Define the base estimator with your tuned parameters\n",
    "base_gb = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.2,\n",
    "    max_depth=3,\n",
    "    min_samples_split=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create an ensemble of multiple instances of your GradientBoostingClassifier\n",
    "ensemble_gb = BaggingClassifier(\n",
    "    estimator=base_gb,\n",
    "    n_estimators=5,          # number of copies\n",
    "    max_samples=0.8,         # each model trains on 80% of the combined data (bootstrapped)\n",
    "    bootstrap=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the ensemble on the combined training set\n",
    "ensemble_gb.fit(X_combined, y_combined)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_test_pred = ensemble_gb.predict(X_test_features)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Ensemble Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Ensemble Precision: {precision:.4f}\")\n",
    "print(f\"Ensemble Recall: {recall:.4f}\")\n",
    "print(f\"Ensemble F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
